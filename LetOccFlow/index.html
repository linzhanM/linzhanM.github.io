<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Let Occ Flow: Self-Supervised Occupancy Flow Prediction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/zju.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Let Occ Flow: Self-Supervised Occupancy Flow Prediction</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=pBEZ7V4AAAAJ&hl=zh-CN">Yili Liu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cIXq7Z4AAAAJ&hl=en">Linzhan Mou</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=4Ry3CKsAAAAJ">Xuan Yu</a>,
              </span>
              <span class="author-block">
                <a href="">Chenrui Han</a>,
              </span>
              <span class="author-block">
                <a href="">Sitong Mao</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=1hI9bqUAAAAJ&hl=en">Rong Xiong</a>,
              </span>
              <span class="author-block">
                <a href="https://ywang-zju.github.io/">Yue Wang</a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Robotics Lab, Zhejiang University</span>
              <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soom)</span>
                  </a>
              </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/demo-kitti.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">Let Occ Flow</span> achieves detailed depth, accurate 3D occupancy, and occupancy flow
          prediction.
        </h2>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Accurate perception of the dynamic environment is a fundamental task
              for autonomous driving and robot systems. This paper introduces <i>Let Occ Flow</i>,
              the first self-supervised work for joint 3D occupancy and occupancy flow prediction
              using only camera inputs, eliminating the need for 3D annotations. Utilizing
              TPV for unified scene representation and deformable attention layers for feature
              aggregation, our approach incorporates a backward-forward temporal attention
              module to capture dynamic object dependencies, followed by a 3D refine module
              for fine-gained volumetric representation. Besides, our method extends differen-
              tiable rendering to 3D volumetric flow fields, leveraging zero-shot 2D segmenta-
              tion and optical flow cues for dynamic decomposition and motion optimization.
              Extensive experiments on nuScenes and KITTI datasets demonstrate the compet-
              itive performance of our approach over prior state-of-the-art methods.
            </p>

            <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Demo Video</h2>
          <div class="publication-video">
            <iframe src="./static/videos/demo.mp4" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <img src="./static/images/pipeline.png" class="interpolation-image"
            alt="Interpolate start reference image." />
          <div class="content has-text-justified">
            <p>
              The overall architecture of Let Occ Flow. We employ deformable-attention layers to integrate multi-view
              image input into TPV representation. The temporal fusion module utilizes BEV-based backward-forward
              attention to fuse temporal feature volumes. The 3D Refine Module further aggregates spatial features and
              upsample the fused volume into a high-solution representation. Then we apply two separate MLP decoders to
              construct volumetric SDF and flow fields, and finally perform self-supervised occupancy flow learning
              utilizing reprojection consistency, optical flow cues, and optional LiDAR ray supervision via
              differentiable rendering.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative and Quantitative Results</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Qualitative results</h3>
          <div class="content has-text-justified">
            <p>
              We show the results for depth estimation, 3D occupancy and occupancy flow prediction on the KITTI dataset.
              Our method can predict visually appealing depth maps, fine-grained occupancy, and accurate dynamic
              decomposition and motion estimation
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column has-text-centered">
              <img src="./static/images/result.png" class="interpolation-image"
                alt="Interpolate start reference image." />
            </div>
          </div>
          <br />
          <!--/ Interpolating. -->


          <!-- Re-rendering. -->
          <h3 class="title is-4">Quantitative results</h3>
          <div class="content has-text-justified">
            <p>
              As reported, our method sets the new state-of-the-art for 3D occupancy prediction and depth estimation
              tasks without any form of 3D supervision compared with other supervised and self-supervised approaches.
              Thanks to our effective spatial-temporal feature aggregation and the integration of optical flow cues for
              supervision, our method greatly enhances the geometric representation capabilities, compared to other
              rendering-based methods.
            </p>
            <img src="./static/images/occ-semantic.png" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>
          <!--/ Re-rendering. -->
          <div class="content has-text-justified">
            <p>
              We report the results of occupancy flow prediction as follows. Compared to other
              rendering-based methods, our approach performs much better on both KITTI-MOT and
              nuScenes dataset, owing to our effective temporal fusion module and flow-oriented optimization 
              strategy. And compared with 3D supervised OccNet, our approach achieves comparable
              performance on nuScenes, validating the effectiveness of our self-supervised training paradigm.
            </p>
            <img src="./static/images/flow-kitti.png" class="interpolation-image"
              alt="Interpolate start reference image." />
            <img src="./static/images/flow-nusc.png" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>

        </div>
      </div>
      <!--/ Animation. -->


      <!-- Concurrent Work. -->
      <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@article{park2021nerfies,
        author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
        title     = {Nerfies: Deformable Neural Radiance Fields},
        journal   = {ICCV},
        year      = {2021},
      }</code></pre> -->
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content is-centered">
          <p>
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>